[{"model": "contenttypes.contenttype", "pk": 1, "fields": {"app_label": "admin", "model": "logentry"}}, {"model": "contenttypes.contenttype", "pk": 2, "fields": {"app_label": "auth", "model": "permission"}}, {"model": "contenttypes.contenttype", "pk": 3, "fields": {"app_label": "auth", "model": "group"}}, {"model": "contenttypes.contenttype", "pk": 4, "fields": {"app_label": "auth", "model": "user"}}, {"model": "contenttypes.contenttype", "pk": 5, "fields": {"app_label": "contenttypes", "model": "contenttype"}}, {"model": "contenttypes.contenttype", "pk": 6, "fields": {"app_label": "sessions", "model": "session"}}, {"model": "contenttypes.contenttype", "pk": 7, "fields": {"app_label": "shortener", "model": "kirrurl"}}, {"model": "contenttypes.contenttype", "pk": 8, "fields": {"app_label": "analytics", "model": "clickevent"}}, {"model": "contenttypes.contenttype", "pk": 9, "fields": {"app_label": "sites", "model": "site"}}, {"model": "contenttypes.contenttype", "pk": 10, "fields": {"app_label": "flatpages", "model": "flatpage"}}, {"model": "contenttypes.contenttype", "pk": 11, "fields": {"app_label": "taggit", "model": "tag"}}, {"model": "contenttypes.contenttype", "pk": 12, "fields": {"app_label": "taggit", "model": "taggeditem"}}, {"model": "contenttypes.contenttype", "pk": 13, "fields": {"app_label": "comments", "model": "comment"}}, {"model": "contenttypes.contenttype", "pk": 14, "fields": {"app_label": "posts", "model": "post"}}, {"model": "contenttypes.contenttype", "pk": 15, "fields": {"app_label": "jet", "model": "bookmark"}}, {"model": "contenttypes.contenttype", "pk": 16, "fields": {"app_label": "jet", "model": "pinnedapplication"}}, {"model": "contenttypes.contenttype", "pk": 17, "fields": {"app_label": "dashboard", "model": "userdashboardmodule"}}, {"model": "sessions.session", "pk": "1x8wg96eq2oci60ngr5q8v9lcj0qmll8", "fields": {"session_data": "OThlYWNjNDYwNDk5N2RlNzk3N2FjYWYyNTRjMDgxMWQwODhlM2E4Njp7Il9hdXRoX3VzZXJfaWQiOiIxIiwiX2F1dGhfdXNlcl9oYXNoIjoiODE5MmRlZWM5NTJjMzkzY2RhNGQwMDQwYjJhYjZmOWNjNzg4ZWVjNyIsIl9hdXRoX3VzZXJfYmFja2VuZCI6ImRqYW5nby5jb250cmliLmF1dGguYmFja2VuZHMuTW9kZWxCYWNrZW5kIn0=", "expire_date": "2016-11-02T16:56:08.151Z"}}, {"model": "sessions.session", "pk": "5zh7axzuqz1y3dbunvzj8tg9fabhd3xj", "fields": {"session_data": "NWU3YzU0ZDFmZGVlMTFlNDc0MmM4N2FiYjExM2M1ZmQ2MzRiY2ZiMDp7Il9hdXRoX3VzZXJfaWQiOiIyIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2NTYxYmE2NmMyNGQ0YWE0ZGY0Y2ZkZTkwMTkzZTBkZjA2NzIyZTU3In0=", "expire_date": "2017-05-02T17:14:20.833Z"}}, {"model": "sessions.session", "pk": "7us86tpqzo3fxz4l8m3exrbi3xt4stzp", "fields": {"session_data": "NWU3YzU0ZDFmZGVlMTFlNDc0MmM4N2FiYjExM2M1ZmQ2MzRiY2ZiMDp7Il9hdXRoX3VzZXJfaWQiOiIyIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2NTYxYmE2NmMyNGQ0YWE0ZGY0Y2ZkZTkwMTkzZTBkZjA2NzIyZTU3In0=", "expire_date": "2017-05-15T18:08:20.375Z"}}, {"model": "sessions.session", "pk": "99nyn3a02nchqgsqzxiwgtw1ji0b60se", "fields": {"session_data": "NWU3YzU0ZDFmZGVlMTFlNDc0MmM4N2FiYjExM2M1ZmQ2MzRiY2ZiMDp7Il9hdXRoX3VzZXJfaWQiOiIyIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2NTYxYmE2NmMyNGQ0YWE0ZGY0Y2ZkZTkwMTkzZTBkZjA2NzIyZTU3In0=", "expire_date": "2017-05-04T06:03:31.015Z"}}, {"model": "sessions.session", "pk": "9cplo3nfx6dd8zgi6vz7pxvrcjk492f1", "fields": {"session_data": "NWU3YzU0ZDFmZGVlMTFlNDc0MmM4N2FiYjExM2M1ZmQ2MzRiY2ZiMDp7Il9hdXRoX3VzZXJfaWQiOiIyIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2NTYxYmE2NmMyNGQ0YWE0ZGY0Y2ZkZTkwMTkzZTBkZjA2NzIyZTU3In0=", "expire_date": "2017-05-15T17:51:27.702Z"}}, {"model": "sessions.session", "pk": "cwuegedjhh5zgc00g9impwkw82xtyq4s", "fields": {"session_data": "NjAwMjlhYjQ1NWZjMDIzZjg2N2QxYjAzMGE2NWFhN2U2NjM1NDcwZjp7Il9hdXRoX3VzZXJfYmFja2VuZCI6ImRqYW5nby5jb250cmliLmF1dGguYmFja2VuZHMuTW9kZWxCYWNrZW5kIiwiX2F1dGhfdXNlcl9pZCI6IjEiLCJfYXV0aF91c2VyX2hhc2giOiI4MTkyZGVlYzk1MmMzOTNjZGE0ZDAwNDBiMmFiNmY5Y2M3ODhlZWM3In0=", "expire_date": "2016-11-03T22:24:40.418Z"}}, {"model": "sessions.session", "pk": "gz5y85yxcis279pp3ui58qebezvvx5h5", "fields": {"session_data": "NWU3YzU0ZDFmZGVlMTFlNDc0MmM4N2FiYjExM2M1ZmQ2MzRiY2ZiMDp7Il9hdXRoX3VzZXJfaWQiOiIyIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2NTYxYmE2NmMyNGQ0YWE0ZGY0Y2ZkZTkwMTkzZTBkZjA2NzIyZTU3In0=", "expire_date": "2017-05-02T17:11:17.380Z"}}, {"model": "sessions.session", "pk": "iqugarg6u75gj7d26xh1tgc1jeex01qu", "fields": {"session_data": "NWU3YzU0ZDFmZGVlMTFlNDc0MmM4N2FiYjExM2M1ZmQ2MzRiY2ZiMDp7Il9hdXRoX3VzZXJfaWQiOiIyIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2NTYxYmE2NmMyNGQ0YWE0ZGY0Y2ZkZTkwMTkzZTBkZjA2NzIyZTU3In0=", "expire_date": "2017-05-05T11:00:44.971Z"}}, {"model": "sessions.session", "pk": "iz9d9z468dcy8ckugmo1uvt842xrqq06", "fields": {"session_data": "NWU3YzU0ZDFmZGVlMTFlNDc0MmM4N2FiYjExM2M1ZmQ2MzRiY2ZiMDp7Il9hdXRoX3VzZXJfaWQiOiIyIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2NTYxYmE2NmMyNGQ0YWE0ZGY0Y2ZkZTkwMTkzZTBkZjA2NzIyZTU3In0=", "expire_date": "2017-05-17T14:07:01.175Z"}}, {"model": "sessions.session", "pk": "rmq8pnyz6r0hlcef8ij0l0kqvduhfeuk", "fields": {"session_data": "NWU3YzU0ZDFmZGVlMTFlNDc0MmM4N2FiYjExM2M1ZmQ2MzRiY2ZiMDp7Il9hdXRoX3VzZXJfaWQiOiIyIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2NTYxYmE2NmMyNGQ0YWE0ZGY0Y2ZkZTkwMTkzZTBkZjA2NzIyZTU3In0=", "expire_date": "2017-05-02T17:26:42.678Z"}}, {"model": "sessions.session", "pk": "u8exssfvv51scmtdejep7gzhhvd5h6i6", "fields": {"session_data": "NWU3YzU0ZDFmZGVlMTFlNDc0MmM4N2FiYjExM2M1ZmQ2MzRiY2ZiMDp7Il9hdXRoX3VzZXJfaWQiOiIyIiwiX2F1dGhfdXNlcl9iYWNrZW5kIjoiZGphbmdvLmNvbnRyaWIuYXV0aC5iYWNrZW5kcy5Nb2RlbEJhY2tlbmQiLCJfYXV0aF91c2VyX2hhc2giOiI2NTYxYmE2NmMyNGQ0YWE0ZGY0Y2ZkZTkwMTkzZTBkZjA2NzIyZTU3In0=", "expire_date": "2017-05-03T10:19:30.455Z"}}, {"model": "sites.site", "pk": 1, "fields": {"domain": "example.com", "name": "example.com"}}, {"model": "taggit.tag", "pk": 1, "fields": {"name": "ArchLinux", "slug": "archlinux"}}, {"model": "taggit.tag", "pk": 2, "fields": {"name": "Python", "slug": "python"}}, {"model": "taggit.tag", "pk": 3, "fields": {"name": "Django", "slug": "django"}}, {"model": "taggit.tag", "pk": 4, "fields": {"name": "Pypi", "slug": "pypi"}}, {"model": "taggit.tag", "pk": 5, "fields": {"name": "Git", "slug": "git"}}, {"model": "taggit.tag", "pk": 6, "fields": {"name": "Gulp", "slug": "gulp"}}, {"model": "taggit.tag", "pk": 7, "fields": {"name": "python", "slug": "python_1"}}, {"model": "taggit.tag", "pk": 8, "fields": {"name": "distutils", "slug": "distutils"}}, {"model": "taggit.tag", "pk": 9, "fields": {"name": "package", "slug": "package"}}, {"model": "taggit.tag", "pk": 10, "fields": {"name": "django", "slug": "django_1"}}, {"model": "taggit.tag", "pk": 11, "fields": {"name": "project-structure", "slug": "project-structure"}}, {"model": "taggit.tag", "pk": 12, "fields": {"name": "gulp", "slug": "gulp_1"}}, {"model": "taggit.tag", "pk": 13, "fields": {"name": "frontend", "slug": "frontend"}}, {"model": "taggit.tag", "pk": 14, "fields": {"name": "webpack", "slug": "webpack"}}, {"model": "taggit.tag", "pk": 15, "fields": {"name": "git", "slug": "git_1"}}, {"model": "taggit.taggeditem", "pk": 1, "fields": {"tag": 8, "content_type": 14, "object_id": 7}}, {"model": "taggit.taggeditem", "pk": 2, "fields": {"tag": 7, "content_type": 14, "object_id": 7}}, {"model": "taggit.taggeditem", "pk": 3, "fields": {"tag": 9, "content_type": 14, "object_id": 6}}, {"model": "taggit.taggeditem", "pk": 4, "fields": {"tag": 7, "content_type": 14, "object_id": 6}}, {"model": "taggit.taggeditem", "pk": 5, "fields": {"tag": 10, "content_type": 14, "object_id": 4}}, {"model": "taggit.taggeditem", "pk": 6, "fields": {"tag": 11, "content_type": 14, "object_id": 4}}, {"model": "taggit.taggeditem", "pk": 7, "fields": {"tag": 12, "content_type": 14, "object_id": 3}}, {"model": "taggit.taggeditem", "pk": 8, "fields": {"tag": 13, "content_type": 14, "object_id": 2}}, {"model": "taggit.taggeditem", "pk": 9, "fields": {"tag": 14, "content_type": 14, "object_id": 2}}, {"model": "taggit.taggeditem", "pk": 10, "fields": {"tag": 15, "content_type": 14, "object_id": 1}}, {"model": "auth.permission", "pk": 1, "fields": {"name": "Can add log entry", "content_type": 1, "codename": "add_logentry"}}, {"model": "auth.permission", "pk": 2, "fields": {"name": "Can change log entry", "content_type": 1, "codename": "change_logentry"}}, {"model": "auth.permission", "pk": 3, "fields": {"name": "Can delete log entry", "content_type": 1, "codename": "delete_logentry"}}, {"model": "auth.permission", "pk": 4, "fields": {"name": "Can add permission", "content_type": 2, "codename": "add_permission"}}, {"model": "auth.permission", "pk": 5, "fields": {"name": "Can change permission", "content_type": 2, "codename": "change_permission"}}, {"model": "auth.permission", "pk": 6, "fields": {"name": "Can delete permission", "content_type": 2, "codename": "delete_permission"}}, {"model": "auth.permission", "pk": 7, "fields": {"name": "Can add group", "content_type": 3, "codename": "add_group"}}, {"model": "auth.permission", "pk": 8, "fields": {"name": "Can change group", "content_type": 3, "codename": "change_group"}}, {"model": "auth.permission", "pk": 9, "fields": {"name": "Can delete group", "content_type": 3, "codename": "delete_group"}}, {"model": "auth.permission", "pk": 10, "fields": {"name": "Can add user", "content_type": 4, "codename": "add_user"}}, {"model": "auth.permission", "pk": 11, "fields": {"name": "Can change user", "content_type": 4, "codename": "change_user"}}, {"model": "auth.permission", "pk": 12, "fields": {"name": "Can delete user", "content_type": 4, "codename": "delete_user"}}, {"model": "auth.permission", "pk": 13, "fields": {"name": "Can add content type", "content_type": 5, "codename": "add_contenttype"}}, {"model": "auth.permission", "pk": 14, "fields": {"name": "Can change content type", "content_type": 5, "codename": "change_contenttype"}}, {"model": "auth.permission", "pk": 15, "fields": {"name": "Can delete content type", "content_type": 5, "codename": "delete_contenttype"}}, {"model": "auth.permission", "pk": 16, "fields": {"name": "Can add session", "content_type": 6, "codename": "add_session"}}, {"model": "auth.permission", "pk": 17, "fields": {"name": "Can change session", "content_type": 6, "codename": "change_session"}}, {"model": "auth.permission", "pk": 18, "fields": {"name": "Can delete session", "content_type": 6, "codename": "delete_session"}}, {"model": "auth.permission", "pk": 19, "fields": {"name": "Can add kirr url", "content_type": 7, "codename": "add_kirrurl"}}, {"model": "auth.permission", "pk": 20, "fields": {"name": "Can change kirr url", "content_type": 7, "codename": "change_kirrurl"}}, {"model": "auth.permission", "pk": 21, "fields": {"name": "Can delete kirr url", "content_type": 7, "codename": "delete_kirrurl"}}, {"model": "auth.permission", "pk": 22, "fields": {"name": "Can add click event", "content_type": 8, "codename": "add_clickevent"}}, {"model": "auth.permission", "pk": 23, "fields": {"name": "Can change click event", "content_type": 8, "codename": "change_clickevent"}}, {"model": "auth.permission", "pk": 24, "fields": {"name": "Can delete click event", "content_type": 8, "codename": "delete_clickevent"}}, {"model": "auth.permission", "pk": 25, "fields": {"name": "Can add site", "content_type": 9, "codename": "add_site"}}, {"model": "auth.permission", "pk": 26, "fields": {"name": "Can change site", "content_type": 9, "codename": "change_site"}}, {"model": "auth.permission", "pk": 27, "fields": {"name": "Can delete site", "content_type": 9, "codename": "delete_site"}}, {"model": "auth.permission", "pk": 28, "fields": {"name": "Can add flat page", "content_type": 10, "codename": "add_flatpage"}}, {"model": "auth.permission", "pk": 29, "fields": {"name": "Can change flat page", "content_type": 10, "codename": "change_flatpage"}}, {"model": "auth.permission", "pk": 30, "fields": {"name": "Can delete flat page", "content_type": 10, "codename": "delete_flatpage"}}, {"model": "auth.permission", "pk": 31, "fields": {"name": "Can add Tag", "content_type": 11, "codename": "add_tag"}}, {"model": "auth.permission", "pk": 32, "fields": {"name": "Can change Tag", "content_type": 11, "codename": "change_tag"}}, {"model": "auth.permission", "pk": 33, "fields": {"name": "Can delete Tag", "content_type": 11, "codename": "delete_tag"}}, {"model": "auth.permission", "pk": 34, "fields": {"name": "Can add Tagged Item", "content_type": 12, "codename": "add_taggeditem"}}, {"model": "auth.permission", "pk": 35, "fields": {"name": "Can change Tagged Item", "content_type": 12, "codename": "change_taggeditem"}}, {"model": "auth.permission", "pk": 36, "fields": {"name": "Can delete Tagged Item", "content_type": 12, "codename": "delete_taggeditem"}}, {"model": "auth.permission", "pk": 37, "fields": {"name": "Can add comment", "content_type": 13, "codename": "add_comment"}}, {"model": "auth.permission", "pk": 38, "fields": {"name": "Can change comment", "content_type": 13, "codename": "change_comment"}}, {"model": "auth.permission", "pk": 39, "fields": {"name": "Can delete comment", "content_type": 13, "codename": "delete_comment"}}, {"model": "auth.permission", "pk": 40, "fields": {"name": "Can add post", "content_type": 14, "codename": "add_post"}}, {"model": "auth.permission", "pk": 41, "fields": {"name": "Can change post", "content_type": 14, "codename": "change_post"}}, {"model": "auth.permission", "pk": 42, "fields": {"name": "Can delete post", "content_type": 14, "codename": "delete_post"}}, {"model": "auth.permission", "pk": 43, "fields": {"name": "Can add bookmark", "content_type": 15, "codename": "add_bookmark"}}, {"model": "auth.permission", "pk": 44, "fields": {"name": "Can change bookmark", "content_type": 15, "codename": "change_bookmark"}}, {"model": "auth.permission", "pk": 45, "fields": {"name": "Can delete bookmark", "content_type": 15, "codename": "delete_bookmark"}}, {"model": "auth.permission", "pk": 46, "fields": {"name": "Can add pinned application", "content_type": 16, "codename": "add_pinnedapplication"}}, {"model": "auth.permission", "pk": 47, "fields": {"name": "Can change pinned application", "content_type": 16, "codename": "change_pinnedapplication"}}, {"model": "auth.permission", "pk": 48, "fields": {"name": "Can delete pinned application", "content_type": 16, "codename": "delete_pinnedapplication"}}, {"model": "auth.permission", "pk": 49, "fields": {"name": "Can add user dashboard module", "content_type": 17, "codename": "add_userdashboardmodule"}}, {"model": "auth.permission", "pk": 50, "fields": {"name": "Can change user dashboard module", "content_type": 17, "codename": "change_userdashboardmodule"}}, {"model": "auth.permission", "pk": 51, "fields": {"name": "Can delete user dashboard module", "content_type": 17, "codename": "delete_userdashboardmodule"}}, {"model": "auth.user", "pk": 1, "fields": {"password": "pbkdf2_sha256$30000$DGIbAHjSvYlm$DN4v+2kXhK8io300C7ESzwz60A3s8tPFsOx/iLH1++I=", "last_login": "2016-10-20T22:24:40.416Z", "is_superuser": true, "username": "cfe", "first_name": "", "last_name": "", "email": "hello@teamcfe.com", "is_staff": true, "is_active": true, "date_joined": "2016-10-19T16:55:44.744Z", "groups": [], "user_permissions": []}}, {"model": "auth.user", "pk": 2, "fields": {"password": "pbkdf2_sha256$24000$f64hD8ztO6bi$PNlLJyMLfkNs1VoFptk2tvLwoIX8+WUWnx3uJ1WE7hA=", "last_login": "2017-05-03T14:07:01.018Z", "is_superuser": true, "username": "vimm0", "first_name": "", "last_name": "", "email": "vimmrana0@gmail.com", "is_staff": true, "is_active": true, "date_joined": "2017-04-18T17:11:05.636Z", "groups": [], "user_permissions": []}}, {"model": "posts.post", "pk": 1, "fields": {"user": 2, "title": "Git Guide", "slug": "git-guide", "image": "", "height_field": 0, "width_field": 0, "content": "# Git guide\r\nFirst of  all install git in your computer\r\n [Install git in window](https://www.atlassian.com/git/tutorials/install-git#windows) and also available for other OS.Please find yourself there :P\r\n1.  Navigate to directory to where you want to clone your git repo and clone repo from https://github.com/vimm0/student_progress_index.git remember project name , then start typing also $ sign signifies starts command\r\n\r\n    ```\r\n    $ git clone https://github.com/vimm0/student_progress_index.git\r\n    ```\r\n\r\n2. Now you have got git repo and if you wish to change file via editor now you are ready to push and also before pushing your stuff\r\n   make sure to type,\r\n\r\n    ```\r\n          $ git pull origin master\r\n\r\n    ```\r\n    Reason to pull before pushing your code is to avoid conflict between contributors. But if you have up-to-date repo now you are ready to push\r\n3. Check the status of the file you changed, but optional if you remember\r\n    ```\r\n          $ git status\r\n\r\n    ```\r\n4. See difference in your terminal if you got confused of what you changed and also it is important to know the changes you have made because you have to write clear message at commit command but optional Ofcorse.\r\n    ```\r\n       $ git diff \"/path/to/file\"\r\n\r\n    ```\r\n5. If you want to add every file at once \"-A\" signifies all.\r\n    ```\r\n          $ git add -A\r\n\r\n    ```\r\n6. If you want to add each file one-by-one,\r\n    ```\r\n          $ git add \"/path/to/file\"\r\n\r\n    ```\r\n7. Commiting the changes of file. \"-m\" signifies message,\r\n    ```\r\n          $ git commit -m \"simple message to clear meaning of update in repo\"\r\n\r\n    ```\r\n8. Finally, push your file by typing,\r\n     ```\r\n          $ git push origin master\r\n\r\n    ```\r\n    And after that you are required to type your git username and password (asked after every push)\r\n          \r\n### Example to clear things up:\r\nThis example commit files one-by-one and also we can commit all by ``git add -A``\r\n \r\n### Check status:\r\n```\r\n  [vimm@ghost src]$ git status\r\n\tOn branch master\r\n\tYour branch is up-to-date with 'origin/master'.\r\n\tChanges not staged for commit:\r\n  (use \"git add <file>...\" to update what will be committed)\r\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n\r\n\tmodified:   requirements.txt\r\n\tmodified:   templates/official/contact.html\r\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n\r\n```\r\n### Add file and commit one-by-one:\r\n```\r\n          (awesome-project) [vimm@ghost src]$ git add \"templates/official/contact.html\"\r\n\t  (awesome-project) [vimm@ghost src]$ git commit -m \"adding information about contact office\"\r\n\t  [master 63f86d9] adding information about contact office\r\n\t  1 file changed, 1 insertion(+), 1 deletion(-)\r\n\r\n    \r\n          (awesome-project) [vimm@ghost src]$ git add \"requirements.txt\" \r\n          (awesome-project) [vimm@ghost src]$ git commit -m \"adding info-based dependency text\"\r\n          [master f8490fd] adding info-based dependency text\r\n          1 file changed, 18 insertions(+), 5 deletions(-)\r\n\r\n```\r\n### Push the changes:\r\n```\r\n          (awesome-project) [vimm@ghost src]$ git push origin master\r\n\t\t  Username for 'https://github.com': vimm0\r\n\t\t  Password for 'https://vimm0@github.com': \r\n\t\t  Counting objects: 8, done.\r\n\t\t  Delta compression using up to 4 threads.\r\n\t\t  Compressing objects: 100% (8/8), done.\r\n\t\t  Writing objects: 100% (8/8), 1.09 KiB | 0 bytes/s, done.\r\n\t\t  Total 8 (delta 4), reused 0 (delta 0)\r\n\t\t  remote: Resolving deltas: 100% (4/4), completed with 3 local objects.\r\n\t\t  To https://github.com/vimm0/awesome-project.git\r\n\t\t   7394d23..f8490fd  master -> master\r\n\r\n```\r\n### Guide from experts no deep shit:\r\n [Git Guide](http://rogerdudler.github.io/git-guide/)", "draft": false, "publish": "2017-01-01", "read_time": 3, "updated": "2017-04-21T11:03:53.086Z", "timestamp": "2017-04-18T17:18:58.246Z"}}, {"model": "posts.post", "pk": 2, "fields": {"user": 2, "title": "web pack", "slug": "web-pack", "image": "", "height_field": 0, "width_field": 0, "content": "#### Web Pack\r\nWebpack is an open-source JavaScript module bundler. Webpack takes modules with dependencies and generates static assets representing those modules.It takes the dependencies and generates a dependency graph allowing you to use a modular approach for your web application development purposes. The bundler can be used from the command line, or can be configured using a config file which is named webpack.config.js.\r\n\r\nYou will need node.js for installing webpack. Another important aspect about webpack is that it is highly extensible by the use of loaders. Loaders allow you to write custom tasks that you want to perform when bundling files together.\r\n\r\nExisting module bundlers are not well suited for big projects (big single page applications). The most pressing reason for developing another module bundler was Code Splitting and that static assets should fit seamlessly together through modularization.\r\n\r\nFor big web apps it's not efficient to put all code into a single file, especially if some blocks of code are only required under some circumstances. Webpack has a feature to split your codebase into \"chunks\" which are loaded on demand. Some other bundlers call them \"layers\", \"rollups\", or \"fragments\". This feature is called \"code splitting\".\r\n\r\nIt's an opt-in feature. You can define split points in your code base. Webpack takes care of the dependencies, output files and runtime stuff.\r\n\r\nTo clarify a common misunderstanding: Code Splitting is not just about extracting common code into a shared chunk. The more notable feature is that Code Splitting can be used to split code into an on demand loaded chunk. This can keep the initial download small and downloads code on demand when requested by the application.\r\n\r\n#### How is webpack different?\r\n\r\n#### Code Splitting\r\n\r\nwebpack has two types of dependencies in its dependency tree: sync and async. Async dependencies act as split points and form a new chunk. After the chunk tree is optimized, a file is emitted for each chunk.\r\n\r\nRead more about Code Splitting.\r\n\r\n#### Loaders\r\n\r\nwebpack can only process JavaScript natively, but loaders are used to transform other resources into JavaScript. By doing so, every resource forms a module.\r\n\r\nRead more about Using loaders and Loaders.\r\n\r\n#### Clever parsing\r\n\r\nwebpack has a clever parser that can process nearly every 3rd party library. It even allows expressions in dependencies such as: require(\"./templates/\" + name + \".jade\"). It handles the most common module styles: CommonJs and AMD.\r\n\r\nRead more about expressions in dependencies, CommonJs and AMD.\r\n\r\n#### Plugin system\r\n\r\nwebpack features a rich plugin system. Most internal features are based on this plugin system. This allows you to customize webpack for your needs and distribute common plugins as open source.\r\n\r\n```\r\nnpm install webpack\r\n\r\n```\r\n\r\n-g if you want to install webpack globally\r\n```\r\nnpm init # (questions to answer)\r\n```\r\n\r\n```\r\nnpm install --save-dev webpack\r\n```\r\n#### project structure:\r\n```\r\n/bin   <--contains binary file i.e bundled files\r\n/src   <--contains source file\r\nwebpack.config.js\r\n```\r\n#### create `webpack.config.js`:\r\n```\r\nmodule.exports = {\r\n     entry: './src/app.js',\r\n     output: {\r\n         path: './bin',\r\n         filename: 'app.bundle.js'\r\n     }\r\n };\r\n ```", "draft": false, "publish": "2017-01-01", "read_time": 3, "updated": "2017-04-21T11:03:43.623Z", "timestamp": "2017-04-18T17:19:30.542Z"}}, {"model": "posts.post", "pk": 3, "fields": {"user": 2, "title": "Gulp for Beginners", "slug": "gulp-for-beginners", "image": "", "height_field": 0, "width_field": 0, "content": "/* This Source Code Form is subject to the terms of the Mozilla Public\r\n * License, v. 2.0. If a copy of the MPL was not distributed with this\r\n * file, You can obtain one at http://mozilla.org/MPL/2.0/. */\r\n\r\n/* global __dirname, require, process */\r\n\r\nvar gulp = require('gulp');\r\nvar karma = require('karma');\r\nvar eslint = require('gulp-eslint');\r\nvar watch = require('gulp-watch');\r\nvar spawn = require('child_process').spawn;\r\n\r\nvar lintPaths = [\r\n    'media/js/**/*.js',\r\n    '!media/js/libs/*.js',\r\n    'tests/unit/spec/**/*.js',\r\n    'gulpfile.js'\r\n];\r\n\r\ngulp.task('serve:backend', function () {\r\n    var devServerPort = process.env.PORT || 8000;\r\n    process.env.PYTHONUNBUFFERED = 1;\r\n    process.env.PYTHONDONTWRITEBITECODE = 1;\r\n    spawn('python', ['manage.py', 'runserver', '0.0.0.0:' + devServerPort], {\r\n        stdio: 'inherit'\r\n    });\r\n});\r\n\r\ngulp.task('media:watch', function () {\r\n    return gulp.src('./media/**/*')\r\n        .pipe(watch('./media/**/*', {\r\n            'verbose': true\r\n        }))\r\n        .pipe(gulp.dest('./static'));\r\n});\r\n\r\ngulp.task('js:test', function(done) {\r\n    new karma.Server({\r\n        configFile: __dirname + '/tests/unit/karma.conf.js',\r\n        singleRun: true\r\n    }, done).start();\r\n});\r\n\r\ngulp.task('js:lint', function() {\r\n    return gulp.src(lintPaths)\r\n        .pipe(eslint())\r\n        .pipe(eslint.format())\r\n        .pipe(eslint.failAfterError());\r\n});\r\n\r\ngulp.task('default', function() {\r\n    gulp.start('serve:backend');\r\n    gulp.start('media:watch');\r\n    gulp.watch(lintPaths, ['js:lint']);\r\n});", "draft": false, "publish": "2017-01-01", "read_time": 1, "updated": "2017-04-21T11:03:27.359Z", "timestamp": "2017-04-18T17:20:55.679Z"}}, {"model": "posts.post", "pk": 4, "fields": {"user": 2, "title": "Django Anatomy", "slug": "django-anatomy", "image": "", "height_field": 0, "width_field": 0, "content": "### Best django project structure\r\n[Learn-Markdown](https://bitbucket.org/tutorials/markdowndemo)\r\n#### standards:\r\n[Django-skel](http://django-skel.readthedocs.io/en/latest/)\r\n```\r\n~/projects/project_name/\r\n\r\ndocs/               # documentation\r\nscripts/\r\n  manage.py         # installed to PATH via setup.py\r\nproject_name/       # project dir (the one which django-admin.py creates)\r\n  apps/             # project-specific applications\r\n    accounts/       # most frequent app, with custom user model\r\n    __init__.py\r\n    ...\r\n  settings/         # settings for different environments, see below\r\n    __init__.py\r\n    production.py\r\n    development.py\r\n    ...\r\n\r\n  __init__.py       # contains project version\r\n  urls.py\r\n  wsgi.py\r\nstatic/             # site-specific static files\r\ntemplates/          # site-specific templates\r\ntests/              # site-specific tests (mostly in-browser ones)\r\ntmp/                # excluded from git\r\nsetup.py\r\nrequirements.txt\r\nrequirements_dev.txt\r\npytest.ini\r\n...\r\n```\r\n\r\n#### Settings\r\n\r\nThe main settings are production ones. Other files (eg. staging.py, development.py) simply import everything from production.py and override only necessary variables.\r\n\r\nFor each environment, there are separate settings files, eg. production, development. I some projects I have also testing (for test runner), staging (as a check before final deploy) and heroku (for deploying to heroku) settings.\r\n\r\n#### Requirements\r\n\r\nI rather specify requirements in setup.py directly. Only those required for development/test environment I have in requirements_dev.txt.\r\n\r\nSome services (eg. heroku) requires to have requirements.txt in root directory.\r\n\r\nsetup.py\r\n\r\nUseful when deploying project using setuptools. It adds manage.py to PATH, so I can run manage.py directly (anywhere).\r\n\r\n#### Project-specific apps\r\n\r\nI used to put these apps into project_name/apps/ directory and import them using relative imports.\r\n\r\n```\r\nTemplates/static/locale/tests files\r\n```\r\n\r\nI put these templates and static files into global templates/static directory, not inside each app. These files are usually edited by people, who doesn't care about project code structure or python at all. If you are full-stack developer working alone or in a small team, you can create per-app templates/static directory. It's really just a matter of taste.\r\n\r\nThe same applies for locale, although sometimes it's convenient to create separate locale directory.\r\n\r\nTests are usually better to place inside each app, but usually there is many integration/functional tests which tests more apps working together, so global tests directory does make sense.\r\n\r\n#### Tmp directory\r\n\r\nThere is temporary directory in project root, excluded from VCS. It's used to store media/static files and sqlite database during development. Everything in tmp could be deleted anytime without any problems.\r\n\r\n#### Virtualenv\r\n\r\nI prefer virtualenvwrapper and place all venvs into ~/.venvs directory, but you could place it inside tmp/ to keep it together.\r\n\r\nProject template\r\n\r\nI've created project template for this setup, django-start-template\r\n\r\n#### Deployment\r\n\r\nDeployment of this project is following:\r\n\r\n```\r\nsource $VENV/bin/activate\r\nexport DJANGO_SETTINGS_MODULE=project_name.settings.production\r\ngit pull\r\npip install -r requirements.txt\r\n```\r\n#### Update database, static files, locales\r\n```\r\nmanage.py syncdb  --noinput\r\nmanage.py migrate\r\nmanage.py collectstatic --noinput\r\nmanage.py makemessages -a\r\nmanage.py compilemessages\r\n```\r\n\r\n#### restart wsgi\r\n```\r\ntouch project_name/wsgi.py\r\n```\r\n\r\nYou can use rsync instead of git, but still you need to run batch of commands to update your environment.\r\n\r\nRecently, I made [django-deploy][2] app, which allows me to run single management command to update environment, but I've used it for one project only and I'm still experimenting with it.\r\n\r\nSketches and drafts\r\n\r\nDraft of templates I place inside global templates/ directory. I guess one can create folder sketches/ in project root, but haven't used it yet.\r\n\r\n#### Pluggable application\r\n\r\nThese apps are usually prepared to publish as open-source. I've taken example below from django-forme\r\n\r\n```~/projects/django-app/\r\n\r\ndocs/\r\napp/\r\ntests/\r\nexample_project/\r\nLICENCE\r\nMANIFEST.in\r\nREADME.md\r\nsetup.py\r\npytest.ini\r\ntox.ini\r\n.travis.yml\r\n...```\r\n\r\nName of directories is clear (I hope). I put test files outside app directory, but it really doesn't matter. It is important to provide README and setup.py, so package is easily installed through pip.", "draft": false, "publish": "2017-01-01", "read_time": 4, "updated": "2017-04-21T11:03:14.927Z", "timestamp": "2017-04-18T17:21:35.738Z"}}, {"model": "posts.post", "pk": 5, "fields": {"user": 2, "title": "laskdjfls", "slug": "laskdjfls", "image": "", "height_field": 0, "width_field": 0, "content": "sfdlaskd fll fjklsakdjlfkas", "draft": false, "publish": "2017-08-12", "read_time": 1, "updated": "2017-04-19T13:48:44.788Z", "timestamp": "2017-04-19T13:48:13.012Z"}}, {"model": "posts.post", "pk": 6, "fields": {"user": 2, "title": "Python Packaging: Hate, hate, hate everywhere", "slug": "python-packaging", "image": "", "height_field": 0, "width_field": 0, "content": "I admit that I was a huge fan of the Python setuptools library for a long time. There was a lot in there which just resonated with how I thought that software development should work. I still think that the design of setuptools is amazing. Nobody would argue that setuptools was flawless and it certainly failed in many regards. The biggest problem probably was that it was build on Python's idiotic import system but there is only so little you can do about that. In general setuptools took the realistic approach to problem-solving: do the best you can do by writing a piece of software scratches the itch without involving a committee or require language changes. That also somewhat explains the second often cited problem of setuptools: that it's a monkeypatch on distutils.\r\n\r\nUpdate: I rarely do updates on my articles but I want to make one thing very clear: the point of this is not to complain about any of the established tools for package management in Python. For a long I did not care about binary distributions because I never had the use case, so I did not even notice that pip did not provide them. Then suddenly the use case came up and I realized that pip removed functionality from plain setuptools and it's very hard (impossible for myself) to bring it back. It's more of an observation how easy it is to miss use cases when replacing tools with something else.\r\n\r\nSetuptools Recap\r\nFor the few Python guys that don't know what setuptools is or why it exists, let me provide you with a very basic recap of what it originally wanted to do. Back in the days someone (and I really don't know who) added a system called \u201cdistutils\u201d to Python. The purpose of that library was to provide functionality to distribute Python libraries. And for its existence we should all be really glad because besides all it's faults it is still providing us with some of the most important bits of Python package distribution. In a nutshell: distutils can make tarballs of Python code, provides you with some simple helpers that help you make an installation script and also knows how to invoke compilers.\r\n\r\nWhat distutils did not do was handling metadata and that's pretty much the reason it exists in the first place. Without metadata there is no dependency tracking. While this is only partially true because there was a very limited amount of metadata even before setuptools available, that metadata disappeared once the library was installed.\r\n\r\nNow to understand why adding metadata is not so trivial with regular Python libraries we first have to understand pth files.\r\n\r\nPTH: The failed Design that enabled it all\r\nI honestly don't know when .pth files were added to Python but I have a theory: I think they were added to make it possible to migrate from modules to packages. At least that would explain why PIL was distributed the way it was. PIL for a long time (and probably still is) was putting a folder called PIL into site packages. However as everybody that worked with PIL knows, you're not importing from the PIL package, you're importing the PIL modules directly. The reason this works is because PIL places a file called pil.pth in the site-packages folder.\r\n\r\nEach line in that file is added to the search path. So far, so uninteresting. There is however another thing with .pth files that is just an example of horrible design: each line that starts with the string import is executed as Python code. This horrible design however enabled setuptools existence to a large degree. setuptools can use these .pth files as a hook to execute custom Python code even if you're not interacting with setuptools directly:\r\n\r\n$ echo $'import sys; sys.stdout.write(\"Hello World!\\\\n\")' > \\\r\n> test/lib/python2.7/site-packages/test.pth\r\n$ test/bin/python\r\nPython 2.7.1 (r271:86832, Jul 31 2011, 19:30:53)\r\n[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nHello World!\r\nAs you can see: the test.pth file is executed when then interpreter starts up. Perfect hook place.\r\n\r\nIntroducing Eggs\r\nIf you're using pip or before you're back in time before setuptools was widespread, your filesystem looked like this after installing libraries:\r\n\r\nlib/\r\n    site-packages/\r\n        flask/\r\n        django/\r\n        genshi/\r\nSetuptools on the other hand did something very different, it placed each installed library in another folder (called an egg) and placed a .pth file that added each of these folders to the python path:\r\n\r\nlib/\r\n    site-packages/\r\n        Flask-1.0.egg/\r\n            flask/\r\n        Django-1.0.egg/\r\n            django/\r\n        Genshi-1.0.egg/\r\n            genshi/\r\n        easy-install.pth\r\nThis behavior enraged a lot of people because it made sys.path very long and hard to look at. Now on the first glance this looks like a horrible idea, why would you do that? Enterprisey!!!11\r\n\r\nHowever in retrospective that is actually the much better solution. And it becomes obvious why that is the case when you look closer at the actual problem and why setuptools works the way it does. The big problem there is that Python developers think of packages as individual things that you can freely nest. They are a form of namespace with specific semantics attached. The way these imports work are convoluted and we won't go into detail here, but the important part is that it's customizable to some degree. This customization however is not exposed through distutils in any way. Now why would you want to customize this?\r\n\r\nThe main reason is splitting up large monolithic packages. For instance take Django. Django as a framework has django as the root packages but it also ships a bunch of addon libraries in django.contrib. If you want to have two packages on PyPI with different release cycles for django and django.contrib.admin, how would you do that with distutils?\r\n\r\nThe answer is: you don't and the main problem is the installation of these. Installing that would mean installing one library physically within another library on the filesystem. Now that sounds like a painless operation but it's not. For instance upgrades of the Django library would also have to take into account that something is now contained within that library.\r\n\r\nThe way setuptools solved that is that it installs all PyPI python packages (that means Flask-1.0.egg intead of flask the importable Python folder) into individual folders and then adds them to the path were appropriate. And it can also hook packages virtually within another package by taking advantage of the fact that pth files can have executable code in it.\r\n\r\nYes, this was not a very clean solution but it worked. Setuptools in a way was the Quake approach to distributing things. You have individual packages of things which then get merged together in a virtual filesystem. In setuptools it's not a virtual filesystem but it's a virtual package tree that is uncoupled from the filesystem.\r\n\r\nAnd with those egg folders it also solved another problem: that there was no place to put metadata. With the added folder, setuptools had found a suitable place to put information that was generated as part of the build process. Next to the importable package within the egg folder there are also text files with the metadata.\r\n\r\nIntroducing the other Eggs\r\nSetuptools was really good with giving the same term to different things, a practice it copied from distutils. Not only do we have Python packages (the things with __init__.py files in it) and PyPI packages (what you download from PyPI) but with setuptools there are also now two kinds of eggs. In the original design of setuptools there was no difference between an egg folder and an egg archive, but over the years and with the introduction of pip that changed.\r\n\r\nNow when people talk about eggs they often talk about the folders on the filesystem with the .egg extension. This however was actually not the interesting part about the original eggs at all. What eggs could do was actually more interesting and something we lost in the transition to pip.\r\n\r\nIf you take a folder with an .egg extension and make a zipfile of it's content and give it the same name as the original folder everything still worked. What was this madness? It's jar files for Python. Why does this work? It works because Python has a default import hook that checks for each file on sys.path if it's a zipfile. If it is, it activates the zip importer and imports from within the zipfile. Due to how zipfiles are structured that's actually a very speedy operation.\r\n\r\nNow that's another thing that people hated about setuptools. Mainly because the Python tracebacks no longer included the source lines with the traceback. However there was no technical limitation for why it should not be able to show the correct line numbers there. It was just a bug in the Python interpreter. Likewise the paths in the tracebacks were wrong too because they often had paths in it that were hard compiled by the person that created the egg file. Again this is an issue with the Python interpreter and not setuptools. For reasons unknown to me, the .pyc build process puts absolute paths into .pyc files sometimes instead of relative to the import path.\r\n\r\nThe other annoyance with eggs is obviously that people tend to do clever things with __file__. Now that's going to break because there are no files. Thankfully setuptools provided a library called pkg_resources which allowed you to extract resources in a distribution agnostic way. So it could give you resources independent of it the resource was in a zipfile or on the filesystem.\r\n\r\nBinary Eggs\r\nAlright. At that point you're asking: what's the real advantage of an egg over just a tarball I install. My tool does the installation, I don't care how it ends up on the filesystem. Fair enough, but there is a huge difference between eggs and tarballs (or source zipfiles for that matter).\r\n\r\nThe difference is that eggs (and I'm talking about actual eggs here, the zipfiles) were usually distributed in binary form (and I recognize that there used to be an issue with ucs2/ucs4 Python builds if you want to be picky. But that does not invalidate the concept!).\r\n\r\ndistutils knows two kinds of distributions: source and binary distributions. Unfortunately binary distributions in distutils don't really work except for redhat (untested, but never heard complaints) and windows. So you can't use distutils binary distributions for instance to install something into your virtualenv on OS X or any flavour of linux. Setuptools however added a new binary distribution target called the egg binary distribution which actually works.\r\n\r\nWhen we talk about a source distribution then we're talking about a tarball that has a setup.py file in it that is executes and installs the library. Binary distributions on the other hand do not have a setup executable any more. You just unpack them and you're done with them. They are also platform specific and you have to make sure you install a binary egg for the correct architecture and ucs size. And then there are other aspects to that too, but they can be ignored to understand the concept.\r\n\r\nWhy Go Binary?\r\nIt's very easy to ignore binary distributions. A lot of code is written in \u201cjust\u201d Python and the compilation is very cheap. The only thing that happens is parsing of the python files and writing out bytecode. Some other things however are more expensive to compile. The one that causes me most troubles is lxml because it takes a minute or two to compile. Now a while ago I ignored that like many other people by just keeping the virtualenv around and I obviously just installed stuff once.\r\n\r\nHowever when we started doing our new server deployment we wanted to have each build revision self contained with its own virtualenv. Now to accomplish our deployment we have two options. We can either go the easy_install + setuptools route or we can go the pip + distribute route. If you're not aware of how pip operates: it basically undoes a bunch of stuff that setuptools does and is unable to install binary eggs.\r\n\r\nThe scenario is very simple:\r\n\r\nHave a big codebase with a bunch of packages that can depend of each other.\r\nThese packages also depend on things that can take a while to compile.\r\nEach build wants to have its own virtualenv\r\nSetuptools Distribution\r\nSo with easy_install and setuptools you can solve this problem very easily. For each package you're dealing with you basically just python setup.py bdist_egg and copy the resulting egg into a folder. Then you start a fileserver that exposes these eggs via HTTP with a fileindex.\r\n\r\nThat out of the way you now make a virtualenv and easy_install --site-url=http://fileserver package-name and you're set.\r\n\r\nPip based Distribution\r\nNow if you do the whole thing through pip instead of setuptool's easy_install command you will notice very quickly that there is no support for binary eggs. Fair enough, so what's the alternative. The way we're doing that currently is making a cached virtualenv and installing things in there. When we deploy new code we copy that virtualenv out, update all the paths in that virtualenv, rerun whatever commands are necessary to build the code (usually just a pip install) and copy it to the target location.\r\n\r\nSince virtualenvs are not relocatable this is what our script does:\r\n\r\nFind all the activation scripts in the bin folder and do a regular expression find for the parts that refer to the virtualenv path and update accordinly.\r\nUpdate all the shebang lines of scripts in the bin folder.\r\nOpen all .pyc files and rewrite the bytecode so that the co_filename is relative instead of absolute.\r\nUpdate symlinks in the virtualenv.\r\nIn theory virtualenvs have a --relocatable flag but that one is heavily broken and conceptionally can't work properly because it uses the system Python interpreter to switch to the intended environment.\r\n\r\nIs all lost?\r\nNo, not at all. Distributing Python code could be much, much worse. I think what can be learned from all that is that it's a better idea to learn all of the design of a project first before attempting to replace it. As you can see from the previous section we're using pip and not easy_install with eggs. Why are we doing that? Because pip did improve certain things over plain setuptools with easy_install and since part of the problem with broken paths is a Python interpreter problem and not one of either setuptools of virtualenv we would have to do path rewriting for pyc files even if we're using binary eggs.\r\n\r\nThe sad aspect is just that we have three competing distribution systems: setuptools with easy_install, distribute with pip as well as the new distutils2 efforts and not one covers all use cases. And I am starting to get the impression that setuptools, despite the fact that it's the oldest still has the best design of all. It ignored theoretical problems and solved practical problems you encounter if you deploy closed source code.\r\n\r\nFor Future Reference\r\nWhat I learned of that personally is not so much anything about packaging Python code but to not make any attempts to replace existing infrastructure without understanding all the reasons that lead to its existence. Also since that happened in the past I think it's a good idea to write down a list of design decisions and use cases and why they exist when I make another open source project in the future. A lot of what went into setuptools can only be understood after a long time of using it because the design is not documented enough.\r\n\r\nAlso there seems to be a lot of domain specific knowledge about tools scattered around. Especially in regards to deploying Python code to a bunch of servers it seems like everybody made his own little tool for it. It seems to me that the theoretical approach that distutils2 is currently taking where there is more design being done on the paper than testing in the real world. Maybe that however is also just a wrong impression I got.\r\n\r\nAll in all, the issue is just too complex and it's easy to miss things when starting from scratch again. Pip was not even from scratch and it forgot about binary distributions and Windows users. As such I suppose it would have been ultimately better to try and repair setuptools with as much hacks as necessary and then rewrite the implementation once all design decisions have been finalized. This seems to have worked good enough for virtualenv which has recently become part of the standard library for Python 3.\r\n\r\nTL;DR: setuptools wrongly got so much hate that tools tried to replace it which did not help much.", "draft": false, "publish": "2017-01-01", "read_time": 15, "updated": "2017-04-21T11:02:47.919Z", "timestamp": "2017-04-20T06:03:51.991Z"}}, {"model": "posts.post", "pk": 7, "fields": {"user": 2, "title": "Differences between distribute, distutils, setuptools and distutils2?", "slug": "distribute-distutils-setuptools-and-distutils", "image": "", "height_field": 0, "width_field": 0, "content": "As of January 2017, all of the other answers to this question are at least two years out-of-date. When you come across advice on Python packaging issues, remember to look at the date of publication, and don't trust out-of-date information.\r\n\r\nThe Python Packaging User Guide is worth a read. Every page has a \"last reviewed\" date displayed, so you can check the recency of the manual, and it's quite comprehensive. The fact that it's hosted on a subdomain of python.org of the Python Software Foundation just adds credence to it. The Project Summaries page is especially relevant here.\r\n\r\nSummary of tools:\r\n\r\nHere's a summary of the Python packaging landscape in January 2017:\r\n\r\nSupported tools:\r\n\r\nDistutils is still the standard tool for packaging in Python. It is included in the standard library (Python 2 and Python 3.0 to 3.6). It is useful for simple Python distributions, but lacks features. It introduces the distutils Python package that can be imported in your setup.py script.\r\nOfficial docs | distutils section of Python Package User Guide\r\nSetuptools was developed to overcome Distutils' limitations, and is not included in the standard library. It introduced a command-line utility called easy_install. It also introduced the setuptools Python package that can be imported in your setup.py script, and the pkg_resources Python package that can be imported in your code to locate data files installed with a distribution. One of its gotchas is that it monkey-patches the distutils Python package. It should work well with pip. It sees regular releases.\r\nOfficial docs | Pypi page | GitHub repo | setuptools section of Python Package User Guide\r\nDeprecated/abandoned tools:\r\n\r\nDistribute was a fork of Setuptools. It shared the same namespace, so if you had Distribute installed, import setuptools would actually import the package distributed with Distribute. Distribute was merged back into Setuptools 0.7, so you don't need to use Distribute any more. In fact, the version on Pypi is just a compatibility layer that installs Setuptools.\r\nDistutils2 was an attempt to take the best of Distutils, Setuptools and Distribute and become the standard tool included in Python's standard library. The idea was that Distutils2 would be distributed for old Python versions, and that Distutils2 would be renamed to packaging for Python 3.3, which would include it in its standard library. These plans did not go as intended, however, and currently, Distutils2 is an abandoned project. The latest release was in March 2012, and its Pypi home page has finally been updated to reflect its death.\r\nAlpha software:\r\n\r\nDistlib is a tool that aims to implement a subset of the previous tools' functionality, but only functionality that is very well-defined in accepted PEPs. It is one of the tools of the PyPA (Python Package Authority), and it should hopefully be included eventually in the Python standard library someday. It is still considered alpha software, so end-users beware.\r\nOfficial docs | Pypi page | Bitbucket repo | distlib section of Python Package User Guide\r\nThere are a couple more tools (eg: Bento), but I won't mention them as they are too obscure or niche or early or undeveloped for this answer post, or else they're not direct alternatives.\r\nRecommendation:\r\n\r\nSo in conclusion, out of all these options, I would recommend Setuptools, unless your requirements are very basic and you only need Distutils. Setuptools works very well with Virtualenv and Pip, tools that I highly recommend. Virtualenv and Pip could both be considered official, as they're part of PyPA, and Python 3 now ships ensurepip (which helps you install pip on some systems).\r\n\r\nIf you're looking into Virtualenv, you might be interested in this question: What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, etc?. (Yes, I know, I groan with you.)\r\n\r\nI\u2019m a distutils maintainer and distutils2/packaging contributor. I did a talk about Python packaging at ConFoo 2011 and these days I\u2019m writing an extended version of it. It\u2019s not published yet, so here are excerpts that should help define things.\r\n\r\nDistutils is the standard tool used for packaging. It works rather well for simple needs, but is limited and not trivial to extend.\r\nSetuptools is a project born from the desire to fill missing distutils functionality and explore new directions. In some subcommunities, it\u2019s a de facto standard. It uses monkey-patching and magic that is frowned upon by Python core developers.\r\nDistribute is a fork of Setuptools that was started by developers feeling that its development pace was too slow and that it was not possible to evolve it. Its development was considerably slowed when distutils2 was started by the same group. 2013-August update: distribute is merged back into setuptools and discontinued.\r\nDistutils2 is a new distutils library, started as a fork of the distutils codebase, with good ideas taken from setup tools (of which some were thoroughly discussed in PEPs), and a basic installer inspired by pip. The actual name you use to import Distutils2 is packaging in the Python 3.3+ standard library, or distutils2 in 2.4+ and 3.1\u20133.2. (A backport will be available soon.) Distutils2 did not make the Python 3.3 release, and it was put on hold.", "draft": false, "publish": "2017-01-01", "read_time": 5, "updated": "2017-04-21T11:02:13.738Z", "timestamp": "2017-04-20T06:08:07.477Z"}}, {"model": "posts.post", "pk": 8, "fields": {"user": 2, "title": "Your First Machine Learning Project in Python Step-By-Step", "slug": "your-first-machine-learning-project-in-python-step-by-step", "image": "", "height_field": 0, "width_field": 0, "content": "Do you want to do machine learning using Python, but you\u2019re having trouble getting started?\r\n\r\nIn this post, you will complete your first machine learning project using Python.\r\n\r\nIn this step-by-step tutorial you will:\r\n\r\nDownload and install Python SciPy and get the most useful package for machine learning in Python.\r\nLoad a dataset and understand it\u2019s structure using statistical summaries and data visualization.\r\nCreate 6 machine learning models, pick the best and build confidence that the accuracy is reliable.\r\nIf you are a machine learning beginner and looking to finally get started using Python, this tutorial was designed for you.\r\n\r\nLet\u2019s get started!\r\n\r\nUpdate Jan/2017: Updated to reflect changes to the scikit-learn API in version 0.18.\r\nUpdated Mar/2017: Added links to help setup your Python environment.\r\nYour First Machine Learning Project in Python Step-By-Step\r\nYour First Machine Learning Project in Python Step-By-Step\r\nPhoto by cosmoflash, some rights reserved.\r\nHow Do You Start Machine Learning in Python?\r\nThe best way to learn machine learning is by designing and completing small projects.\r\n\r\nPython Can Be Intimidating When Getting Started\r\n\r\nPython is a popular and powerful interpreted language. Unlike R, Python is a complete language and platform that you can use for both research and development and developing production systems.\r\n\r\nThere are also a lot of modules and libraries to choose from, providing multiple ways to do each task. It can feel overwhelming.\r\n\r\nThe best way to get started using Python for machine learning is to complete a project.\r\n\r\nIt will force you to install and start the Python interpreter (at the very least).\r\nIt will given you a bird\u2019s eye view of how to step through a small project.\r\nIt will give you confidence, maybe to go on to your own small projects.\r\nBeginners Need A Small End-to-End Project\r\n\r\nBooks and courses are frustrating. They give you lots of recipes and snippets, but you never get to see how they all fit together.\r\n\r\nWhen you are applying machine learning to your own datasets, you are working on a project.\r\n\r\nA machine learning project may not be linear, but it has a number of well known steps:\r\n\r\nDefine Problem.\r\nPrepare Data.\r\nEvaluate Algorithms.\r\nImprove Results.\r\nPresent Results.\r\nThe best way to really come to terms with a new platform or tool is to work through a machine learning project end-to-end and cover the key steps. Namely, from loading data, summarizing data, evaluating algorithms and making some predictions.\r\n\r\nIf you can do that, you have a template that you can use on dataset after dataset. You can fill in the gaps such as further data preparation and improving result tasks later, once you have more confidence.\r\n\r\nHello World of Machine Learning\r\n\r\nThe best small project to start with on a new tool is the classification of iris flowers (e.g. the iris dataset).\r\n\r\nThis is a good project because it is so well understood.\r\n\r\nAttributes are numeric so you have to figure out how to load and handle data.\r\nIt is a classification problem, allowing you to practice with perhaps an easier type of supervised learning algorithm.\r\nIt is a multi-class classification problem (multi-nominal) that may require some specialized handling.\r\nIt only has 4 attributes and 150 rows, meaning it is small and easily fits into memory (and a screen or A4 page).\r\nAll of the numeric attributes are in the same units and the same scale, not requiring any special scaling or transforms to get started.\r\nLet\u2019s get started with your hello world machine learning project in Python.\r\n\r\nMachine Learning in Python: Step-By-Step Tutorial\r\n(start here)\r\nIn this section, we are going to work through a small machine learning project end-to-end.\r\n\r\nHere is an overview of what we are going to cover:\r\n\r\nInstalling the Python and SciPy platform.\r\nLoading the dataset.\r\nSummarizing the dataset.\r\nVisualizing the dataset.\r\nEvaluating some algorithms.\r\nMaking some predictions.\r\nTake your time. Work through each step.\r\n\r\nTry to type in the commands yourself or copy-and-paste the commands to speed things up.\r\n\r\nIf you have any questions at all, please leave a comment at the bottom of the post.\r\n\r\nBeat Information Overload and Master the Fastest Growing Platform of Machine Learning Pros\r\n\r\nMachine Learning Mastery With Python Mini-CourseGet my free Machine Learning With Python mini course and start loading your own datasets from CSV in just 1 hour.\r\n\r\nDaily lessons in your inbox for 14 days, and a Machine-Learning-With-Python \u201cCheat Sheet\u201d you can download right now.\r\nDownload Your FREE Mini-Course >>\r\n \r\n\r\n1. Downloading, Installing and Starting Python SciPy\r\nGet the Python and SciPy platform installed on your system if it is not already.\r\n\r\nI do not want to cover this in great detail, because others already have. This is already pretty straightforward, especially if you are a developer. If you do need help, ask a question in the comments.\r\n\r\n1.1 Install SciPy Libraries\r\n\r\nThis tutorial assumes Python version 2.7 or 3.5.\r\n\r\nThere are 5 key libraries that you will need to install. Below is a list of the Python SciPy libraries required for this tutorial:\r\n\r\nscipy\r\nnumpy\r\nmatplotlib\r\npandas\r\nsklearn\r\nThere are many ways to install these libraries. My best advice is to pick one method then be consistent in installing each library.\r\n\r\nThe scipy installation page provides excellent instructions for installing the above libraries on multiple different platforms, such as Linux, mac OS X and Windows. If you have any doubts or questions, refer to this guide, it has been followed by thousands of people.\r\n\r\nOn Mac OS X, you can use macports to install Python 2.7 and these libraries. For more information on macports, see the homepage.\r\nOn Linux you can use your package manager, such as yum on Fedora to install RPMs.\r\nIf you are on Windows or you are not confident, I would recommend installing the free version of Anaconda that includes everything you need.\r\n\r\nNote: This tutorial assumes you have scikit-learn version 0.18 or higher installed.\r\n\r\nNeed more help? See one of these tutorials:\r\n\r\nHow to Setup a Python Environment for Machine Learning and Deep Learning with Anaconda\r\nHow to Create a Linux Virtual Machine For Machine Learning Development With Python 3\r\n1.2 Start Python and Check Versions\r\n\r\nIt is a good idea to make sure your Python environment was installed successfully and is working as expected.\r\n\r\nThe script below will help you test out your environment. It imports each library required in this tutorial and prints the version.\r\n\r\nOpen a command line and start the python interpreter:\r\n\r\n\r\npython\r\n1\r\npython\r\nI recommend working directly in the interpreter or writing your scripts and running them on the command line rather than big editors and IDEs. Keep things simple and focus on the machine learning not the toolchain.\r\n\r\nType or copy and paste the following script:\r\n\r\n\r\n# Check the versions of libraries\r\n\r\n# Python version\r\nimport sys\r\nprint('Python: {}'.format(sys.version))\r\n# scipy\r\nimport scipy\r\nprint('scipy: {}'.format(scipy.__version__))\r\n# numpy\r\nimport numpy\r\nprint('numpy: {}'.format(numpy.__version__))\r\n# matplotlib\r\nimport matplotlib\r\nprint('matplotlib: {}'.format(matplotlib.__version__))\r\n# pandas\r\nimport pandas\r\nprint('pandas: {}'.format(pandas.__version__))\r\n# scikit-learn\r\nimport sklearn\r\nprint('sklearn: {}'.format(sklearn.__version__))\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n# Check the versions of libraries\r\n \r\n# Python version\r\nimport sys\r\nprint('Python: {}'.format(sys.version))\r\n# scipy\r\nimport scipy\r\nprint('scipy: {}'.format(scipy.__version__))\r\n# numpy\r\nimport numpy\r\nprint('numpy: {}'.format(numpy.__version__))\r\n# matplotlib\r\nimport matplotlib\r\nprint('matplotlib: {}'.format(matplotlib.__version__))\r\n# pandas\r\nimport pandas\r\nprint('pandas: {}'.format(pandas.__version__))\r\n# scikit-learn\r\nimport sklearn\r\nprint('sklearn: {}'.format(sklearn.__version__))\r\nHere is the output I get on my OS X workstation:\r\n\r\n\r\nPython: 2.7.11 (default, Mar  1 2016, 18:40:10) \r\n[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)]\r\nscipy: 0.17.0\r\nnumpy: 1.10.4\r\nmatplotlib: 1.5.1\r\npandas: 0.17.1\r\nsklearn: 0.18.1\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\nPython: 2.7.11 (default, Mar  1 2016, 18:40:10) \r\n[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)]\r\nscipy: 0.17.0\r\nnumpy: 1.10.4\r\nmatplotlib: 1.5.1\r\npandas: 0.17.1\r\nsklearn: 0.18.1\r\nCompare the above output to your versions.\r\n\r\nIdeally, your versions should match or be more recent. The APIs do not change quickly, so do not be too concerned if you are a few versions behind, Everything in this tutorial will very likely still work for you.\r\n\r\nIf you get an error, stop. Now is the time to fix it.\r\n\r\nIf you cannot run the above script cleanly you will not be able to complete this tutorial.\r\n\r\nMy best advice is to Google search for your error message or post a question on Stack Exchange.\r\n\r\n2. Load The Data\r\nWe are going to use the iris flowers dataset. This dataset is famous because it is used as the \u201chello world\u201d dataset in machine learning and statistics by pretty much everyone.\r\n\r\nThe dataset contains 150 observations of iris flowers. There are four columns of measurements of the flowers in centimeters. The fifth column is the species of the flower observed. All observed flowers belong to one of three species.\r\n\r\nYou can learn more about this dataset on Wikipedia.\r\n\r\nIn this step we are going to load the iris data from CSV file URL.\r\n\r\n2.1 Import libraries\r\n\r\nFirst, let\u2019s import all of the modules, functions and objects we are going to use in this tutorial.\r\n\r\n\r\n# Load libraries\r\nimport pandas\r\nfrom pandas.tools.plotting import scatter_matrix\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn import model_selection\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.svm import SVC\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n# Load libraries\r\nimport pandas\r\nfrom pandas.tools.plotting import scatter_matrix\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn import model_selection\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.svm import SVC\r\nEverything should load without error. If you have an error, stop. You need a working SciPy environment before continuing. See the advice above about setting up your environment.\r\n\r\n2.2 Load Dataset\r\n\r\nWe can load the data directly from the UCI Machine Learning repository.\r\n\r\nWe are using pandas to load the data. We will also use pandas next to explore the data both with descriptive statistics and data visualization.\r\n\r\nNote that we are specifying the names of each column when loading the data. This will help later when we explore the data.\r\n\r\n\r\n# Load dataset\r\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\r\nnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\r\ndataset = pandas.read_csv(url, names=names)\r\n1\r\n2\r\n3\r\n4\r\n# Load dataset\r\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\r\nnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\r\ndataset = pandas.read_csv(url, names=names)\r\nThe dataset should load without incident.\r\n\r\nIf you do have network problems, you can download the iris.data file into your working directory and load it using the same method, changing URL to the local file name.\r\n\r\n3. Summarize the Dataset\r\nNow it is time to take a look at the data.\r\n\r\nIn this step we are going to take a look at the data a few different ways:\r\n\r\nDimensions of the dataset.\r\nPeek at the data itself.\r\nStatistical summary of all attributes.\r\nBreakdown of the data by the class variable.\r\nDon\u2019t worry, each look at the data is one command. These are useful commands that you can use again and again on future projects.\r\n\r\n3.1 Dimensions of Dataset\r\n\r\nWe can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\r\n\r\n\r\n# shape\r\nprint(dataset.shape)\r\n1\r\n2\r\n# shape\r\nprint(dataset.shape)\r\nYou should see 150 instances and 5 attributes:\r\n\r\n\r\n(150, 5)\r\n1\r\n(150, 5)\r\n3.2 Peek at the Data\r\n\r\nIt is also always a good idea to actually eyeball your data.\r\n\r\n\r\n# head\r\nprint(dataset.head(20))\r\n1\r\n2\r\n# head\r\nprint(dataset.head(20))\r\nYou should see the first 20 rows of the data:\r\n\r\n\r\n    sepal-length  sepal-width  petal-length  petal-width        class\r\n0            5.1          3.5           1.4          0.2  Iris-setosa\r\n1            4.9          3.0           1.4          0.2  Iris-setosa\r\n2            4.7          3.2           1.3          0.2  Iris-setosa\r\n3            4.6          3.1           1.5          0.2  Iris-setosa\r\n4            5.0          3.6           1.4          0.2  Iris-setosa\r\n5            5.4          3.9           1.7          0.4  Iris-setosa\r\n6            4.6          3.4           1.4          0.3  Iris-setosa\r\n7            5.0          3.4           1.5          0.2  Iris-setosa\r\n8            4.4          2.9           1.4          0.2  Iris-setosa\r\n9            4.9          3.1           1.5          0.1  Iris-setosa\r\n10           5.4          3.7           1.5          0.2  Iris-setosa\r\n11           4.8          3.4           1.6          0.2  Iris-setosa\r\n12           4.8          3.0           1.4          0.1  Iris-setosa\r\n13           4.3          3.0           1.1          0.1  Iris-setosa\r\n14           5.8          4.0           1.2          0.2  Iris-setosa\r\n15           5.7          4.4           1.5          0.4  Iris-setosa\r\n16           5.4          3.9           1.3          0.4  Iris-setosa\r\n17           5.1          3.5           1.4          0.3  Iris-setosa\r\n18           5.7          3.8           1.7          0.3  Iris-setosa\r\n19           5.1          3.8           1.5          0.3  Iris-setosa\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n    sepal-length  sepal-width  petal-length  petal-width        class\r\n0            5.1          3.5           1.4          0.2  Iris-setosa\r\n1            4.9          3.0           1.4          0.2  Iris-setosa\r\n2            4.7          3.2           1.3          0.2  Iris-setosa\r\n3            4.6          3.1           1.5          0.2  Iris-setosa\r\n4            5.0          3.6           1.4          0.2  Iris-setosa\r\n5            5.4          3.9           1.7          0.4  Iris-setosa\r\n6            4.6          3.4           1.4          0.3  Iris-setosa\r\n7            5.0          3.4           1.5          0.2  Iris-setosa\r\n8            4.4          2.9           1.4          0.2  Iris-setosa\r\n9            4.9          3.1           1.5          0.1  Iris-setosa\r\n10           5.4          3.7           1.5          0.2  Iris-setosa\r\n11           4.8          3.4           1.6          0.2  Iris-setosa\r\n12           4.8          3.0           1.4          0.1  Iris-setosa\r\n13           4.3          3.0           1.1          0.1  Iris-setosa\r\n14           5.8          4.0           1.2          0.2  Iris-setosa\r\n15           5.7          4.4           1.5          0.4  Iris-setosa\r\n16           5.4          3.9           1.3          0.4  Iris-setosa\r\n17           5.1          3.5           1.4          0.3  Iris-setosa\r\n18           5.7          3.8           1.7          0.3  Iris-setosa\r\n19           5.1          3.8           1.5          0.3  Iris-setosa\r\n3.3 Statistical Summary\r\n\r\nNow we can take a look at a summary of each attribute.\r\n\r\nThis includes the count, mean, the min and max values as well as some percentiles.\r\n\r\n\r\n# descriptions\r\nprint(dataset.describe())\r\n1\r\n2\r\n# descriptions\r\nprint(dataset.describe())\r\nWe can see that all of the numerical values have the same scale (centimeters) and similar ranges between 0 and 8 centimeters.\r\n\r\n\r\n       sepal-length  sepal-width  petal-length  petal-width\r\ncount    150.000000   150.000000    150.000000   150.000000\r\nmean       5.843333     3.054000      3.758667     1.198667\r\nstd        0.828066     0.433594      1.764420     0.763161\r\nmin        4.300000     2.000000      1.000000     0.100000\r\n25%        5.100000     2.800000      1.600000     0.300000\r\n50%        5.800000     3.000000      4.350000     1.300000\r\n75%        6.400000     3.300000      5.100000     1.800000\r\nmax        7.900000     4.400000      6.900000     2.500000\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n       sepal-length  sepal-width  petal-length  petal-width\r\ncount    150.000000   150.000000    150.000000   150.000000\r\nmean       5.843333     3.054000      3.758667     1.198667\r\nstd        0.828066     0.433594      1.764420     0.763161\r\nmin        4.300000     2.000000      1.000000     0.100000\r\n25%        5.100000     2.800000      1.600000     0.300000\r\n50%        5.800000     3.000000      4.350000     1.300000\r\n75%        6.400000     3.300000      5.100000     1.800000\r\nmax        7.900000     4.400000      6.900000     2.500000\r\n3.4 Class Distribution\r\n\r\nLet\u2019s now take a look at the number of instances (rows) that belong to each class. We can view this as an absolute count.\r\n\r\n\r\n# class distribution\r\nprint(dataset.groupby('class').size())\r\n1\r\n2\r\n# class distribution\r\nprint(dataset.groupby('class').size())\r\nWe can see that each class has the same number of instances (50 or 33% of the dataset).\r\n\r\n\r\nclass\r\nIris-setosa        50\r\nIris-versicolor    50\r\nIris-virginica     50\r\n1\r\n2\r\n3\r\n4\r\nclass\r\nIris-setosa        50\r\nIris-versicolor    50\r\nIris-virginica     50\r\n4. Data Visualization\r\nWe now have a basic idea about the data. We need to extend that with some visualizations.\r\n\r\nWe are going to look at two types of plots:\r\n\r\nUnivariate plots to better understand each attribute.\r\nMultivariate plots to better understand the relationships between attributes.\r\n4.1 Univariate Plots\r\n\r\nWe start with some univariate plots, that is, plots of each individual variable.\r\n\r\nGiven that the input variables are numeric, we can create box and whisker plots of each.\r\n\r\n\r\n# box and whisker plots\r\ndataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\r\nplt.show()\r\n1\r\n2\r\n3\r\n# box and whisker plots\r\ndataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\r\nplt.show()\r\nThis gives us a much clearer idea of the distribution of the input attributes:\r\n\r\nBox and Whisker Plots\r\nBox and Whisker Plots\r\nWe can also create a histogram of each input variable to get an idea of the distribution.\r\n\r\n\r\n# histograms\r\ndataset.hist()\r\nplt.show()\r\n1\r\n2\r\n3\r\n# histograms\r\ndataset.hist()\r\nplt.show()\r\nIt looks like perhaps two of the input variables have a Gaussian distribution. This is useful to note as we can use algorithms that can exploit this assumption.\r\n\r\nHistogram Plots\r\nHistogram Plots\r\n4.2 Multivariate Plots\r\n\r\nNow we can look at the interactions between the variables.\r\n\r\nFirst, let\u2019s look at scatterplots of all pairs of attributes. This can be helpful to spot structured relationships between input variables.\r\n\r\n\r\n# scatter plot matrix\r\nscatter_matrix(dataset)\r\nplt.show()\r\n1\r\n2\r\n3\r\n# scatter plot matrix\r\nscatter_matrix(dataset)\r\nplt.show()\r\nNote the diagonal grouping of some pairs of attributes. This suggests a high correlation and a predictable relationship.\r\n\r\nScattplot Matrix\r\nScatterplot Matrix\r\n5. Evaluate Some Algorithms\r\nNow it is time to create some models of the data and estimate their accuracy on unseen data.\r\n\r\nHere is what we are going to cover in this step:\r\n\r\nSeparate out a validation dataset.\r\nSet-up the test harness to use 10-fold cross validation.\r\nBuild 5 different models to predict species from flower measurements\r\nSelect the best model.\r\n5.1 Create a Validation Dataset\r\n\r\nWe need to know that the model we created is any good.\r\n\r\nLater, we will use statistical methods to estimate the accuracy of the models that we create on unseen data. We also want a more concrete estimate of the accuracy of the best model on unseen data by evaluating it on actual unseen data.\r\n\r\nThat is, we are going to hold back some data that the algorithms will not get to see and we will use this data to get a second and independent idea of how accurate the best model might actually be.\r\n\r\nWe will split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as a validation dataset.\r\n\r\n\r\n# Split-out validation dataset\r\narray = dataset.values\r\nX = array[:,0:4]\r\nY = array[:,4]\r\nvalidation_size = 0.20\r\nseed = 7\r\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n# Split-out validation dataset\r\narray = dataset.values\r\nX = array[:,0:4]\r\nY = array[:,4]\r\nvalidation_size = 0.20\r\nseed = 7\r\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\r\nYou now have training data in the X_train and Y_train for preparing models and a X_validation and Y_validation sets that we can use later.\r\n\r\n5.2 Test Harness\r\n\r\nWe will use 10-fold cross validation to estimate accuracy.\r\n\r\nThis will split our dataset into 10 parts, train on 9 and test on 1 and repeat for all combinations of train-test splits.\r\n\r\n\r\n# Test options and evaluation metric\r\nseed = 7\r\nscoring = 'accuracy'\r\n1\r\n2\r\n3\r\n# Test options and evaluation metric\r\nseed = 7\r\nscoring = 'accuracy'\r\nWe are using the metric of \u2018accuracy\u2018 to evaluate models. This is a ratio of the number of correctly predicted instances in divided by the total number of instances in the dataset multiplied by 100 to give a percentage (e.g. 95% accurate). We will be using the scoring variable when we run build and evaluate each model next.\r\n\r\n5.3 Build Models\r\n\r\nWe don\u2019t know which algorithms would be good on this problem or what configurations to use. We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we are expecting generally good results.\r\n\r\nLet\u2019s evaluate 6 different algorithms:\r\n\r\nLogistic Regression (LR)\r\nLinear Discriminant Analysis (LDA)\r\nK-Nearest Neighbors (KNN).\r\nClassification and Regression Trees (CART).\r\nGaussian Naive Bayes (NB).\r\nSupport Vector Machines (SVM).\r\nThis is a good mixture of simple linear (LR and LDA), nonlinear (KNN, CART, NB and SVM) algorithms. We reset the random number seed before each run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable.\r\n\r\nLet\u2019s build and evaluate our five models:\r\n\r\n\r\n# Spot Check Algorithms\r\nmodels = []\r\nmodels.append(('LR', LogisticRegression()))\r\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\r\nmodels.append(('KNN', KNeighborsClassifier()))\r\nmodels.append(('CART', DecisionTreeClassifier()))\r\nmodels.append(('NB', GaussianNB()))\r\nmodels.append(('SVM', SVC()))\r\n# evaluate each model in turn\r\nresults = []\r\nnames = []\r\nfor name, model in models:\r\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\r\n\tcv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\r\n\tresults.append(cv_results)\r\n\tnames.append(name)\r\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\r\n\tprint(msg)\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n# Spot Check Algorithms\r\nmodels = []\r\nmodels.append(('LR', LogisticRegression()))\r\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\r\nmodels.append(('KNN', KNeighborsClassifier()))\r\nmodels.append(('CART', DecisionTreeClassifier()))\r\nmodels.append(('NB', GaussianNB()))\r\nmodels.append(('SVM', SVC()))\r\n# evaluate each model in turn\r\nresults = []\r\nnames = []\r\nfor name, model in models:\r\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\r\n\tcv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\r\n\tresults.append(cv_results)\r\n\tnames.append(name)\r\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\r\n\tprint(msg)\r\n5.3 Select Best Model\r\n\r\nWe now have 6 models and accuracy estimations for each. We need to compare the models to each other and select the most accurate.\r\n\r\nRunning the example above, we get the following raw results:\r\n\r\n\r\nLR: 0.966667 (0.040825)\r\nLDA: 0.975000 (0.038188)\r\nKNN: 0.983333 (0.033333)\r\nCART: 0.975000 (0.038188)\r\nNB: 0.975000 (0.053359)\r\nSVM: 0.981667 (0.025000)\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\nLR: 0.966667 (0.040825)\r\nLDA: 0.975000 (0.038188)\r\nKNN: 0.983333 (0.033333)\r\nCART: 0.975000 (0.038188)\r\nNB: 0.975000 (0.053359)\r\nSVM: 0.981667 (0.025000)\r\nWe can see that it looks like KNN has the largest estimated accuracy score.\r\n\r\nWe can also create a plot of the model evaluation results and compare the spread and the mean accuracy of each model. There is a population of accuracy measures for each algorithm because each algorithm was evaluated 10 times (10 fold cross validation).\r\n\r\n\r\n# Compare Algorithms\r\nfig = plt.figure()\r\nfig.suptitle('Algorithm Comparison')\r\nax = fig.add_subplot(111)\r\nplt.boxplot(results)\r\nax.set_xticklabels(names)\r\nplt.show()\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n# Compare Algorithms\r\nfig = plt.figure()\r\nfig.suptitle('Algorithm Comparison')\r\nax = fig.add_subplot(111)\r\nplt.boxplot(results)\r\nax.set_xticklabels(names)\r\nplt.show()\r\nYou can see that the box and whisker plots are squashed at the top of the range, with many samples achieving 100% accuracy.\r\n\r\nCompare Algorithm Accuracy\r\nCompare Algorithm Accuracy\r\n6. Make Predictions\r\nThe KNN algorithm was the most accurate model that we tested. Now we want to get an idea of the accuracy of the model on our validation set.\r\n\r\nThis will give us an independent final check on the accuracy of the best model. It is valuable to keep a validation set just in case you made a slip during training, such as overfitting to the training set or a data leak. Both will result in an overly optimistic result.\r\n\r\nWe can run the KNN model directly on the validation set and summarize the results as a final accuracy score, a confusion matrix and a classification report.\r\n\r\n\r\n# Make predictions on validation dataset\r\nknn = KNeighborsClassifier()\r\nknn.fit(X_train, Y_train)\r\npredictions = knn.predict(X_validation)\r\nprint(accuracy_score(Y_validation, predictions))\r\nprint(confusion_matrix(Y_validation, predictions))\r\nprint(classification_report(Y_validation, predictions))\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n# Make predictions on validation dataset\r\nknn = KNeighborsClassifier()\r\nknn.fit(X_train, Y_train)\r\npredictions = knn.predict(X_validation)\r\nprint(accuracy_score(Y_validation, predictions))\r\nprint(confusion_matrix(Y_validation, predictions))\r\nprint(classification_report(Y_validation, predictions))\r\nWe can see that the accuracy is 0.9 or 90%. The confusion matrix provides an indication of the three errors made. Finally, the classification report provides a breakdown of each class by precision, recall, f1-score and support showing excellent results (granted the validation dataset was small).\r\n\r\n\r\n0.9\r\n\r\n[[ 7  0  0]\r\n [ 0 11  1]\r\n [ 0  2  9]]\r\n\r\n             precision    recall  f1-score   support\r\n\r\nIris-setosa       1.00      1.00      1.00         7\r\nIris-versicolor   0.85      0.92      0.88        12\r\nIris-virginica    0.90      0.82      0.86        11\r\n\r\navg / total       0.90      0.90      0.90        30\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n0.9\r\n \r\n[[ 7  0  0]\r\n [ 0 11  1]\r\n [ 0  2  9]]\r\n \r\n             precision    recall  f1-score   support\r\n \r\nIris-setosa       1.00      1.00      1.00         7\r\nIris-versicolor   0.85      0.92      0.88        12\r\nIris-virginica    0.90      0.82      0.86        11\r\n \r\navg / total       0.90      0.90      0.90        30\r\nYou Can Do Machine Learning in Python\r\nWork through the tutorial above. It will take you 5-to-10 minutes, max!\r\n\r\nYou do not need to understand everything. (at least not right now) Your goal is to run through the tutorial end-to-end and get a result. You do not need to understand everything on the first pass. List down your questions as you go. Make heavy use of the help(\u201cFunctionName\u201d) help syntax in Python to learn about all of the functions that you\u2019re using.\r\n\r\nYou do not need to know how the algorithms work. It is important to know about the limitations and how to configure machine learning algorithms. But learning about algorithms can come later. You need to build up this algorithm knowledge slowly over a long period of time. Today, start off by getting comfortable with the platform.\r\n\r\nYou do not need to be a Python programmer. The syntax of the Python language can be intuitive if you are new to it. Just like other languages, focus on function calls (e.g. function()) and assignments (e.g. a = \u201cb\u201d). This will get you most of the way. You are a developer, you know how to pick up the basics of a language real fast. Just get started and dive into the details later.\r\n\r\nYou do not need to be a machine learning expert. You can learn about the benefits and limitations of various algorithms later, and there are plenty of posts that you can read later to brush up on the steps of a machine learning project and the importance of evaluating accuracy using cross validation.\r\n\r\nWhat about other steps in a machine learning project. We did not cover all of the steps in a machine learning project because this is your first project and we need to focus on the key steps. Namely, loading data, looking at the data, evaluating some algorithms and making some predictions. In later tutorials we can look at other data preparation and result improvement tasks.\r\n\r\nSummary\r\nIn this post, you discovered step-by-step how to complete your first machine learning project in Python.\r\n\r\nYou discovered that completing a small end-to-end project from loading the data to making predictions is the best way to get familiar with a new platform.\r\n\r\nYour Next Step\r\n\r\nDo you work through the tutorial?\r\n\r\nWork through the above tutorial.\r\nList any questions you have.\r\nSearch or research the answers.\r\nRemember, you can use the help(\u201cFunctionName\u201d) in Python to get help on any function.\r\nDo you have a question? Post it in the comments below.", "draft": false, "publish": "2017-01-01", "read_time": 25, "updated": "2017-05-03T14:08:26.610Z", "timestamp": "2017-05-03T14:08:26.610Z"}}, {"model": "admin.logentry", "pk": 1, "fields": {"action_time": "2016-10-19T16:56:27.660Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 1, "change_message": "[{\"added\": {}}]"}}, {"model": "admin.logentry", "pk": 2, "fields": {"action_time": "2016-10-19T17:18:50.317Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 3, "fields": {"action_time": "2016-10-19T17:18:56.740Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 4, "fields": {"action_time": "2016-10-19T17:18:57.979Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 5, "fields": {"action_time": "2016-10-19T17:18:58.315Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 6, "fields": {"action_time": "2016-10-19T17:18:58.529Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 7, "fields": {"action_time": "2016-10-19T17:18:58.720Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 8, "fields": {"action_time": "2016-10-19T17:19:58.934Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 9, "fields": {"action_time": "2016-10-19T17:19:59.370Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 10, "fields": {"action_time": "2016-10-19T17:19:59.830Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 11, "fields": {"action_time": "2016-10-19T17:20:00.200Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 12, "fields": {"action_time": "2016-10-19T17:23:13.684Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[{\"changed\": {\"fields\": [\"shortcode\"]}}]"}}, {"model": "admin.logentry", "pk": 13, "fields": {"action_time": "2016-10-19T17:25:09.472Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 14, "fields": {"action_time": "2016-10-19T17:25:17.288Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[{\"changed\": {\"fields\": [\"shortcode\"]}}]"}}, {"model": "admin.logentry", "pk": 15, "fields": {"action_time": "2016-10-19T17:25:23.028Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[{\"changed\": {\"fields\": [\"shortcode\"]}}]"}}, {"model": "admin.logentry", "pk": 16, "fields": {"action_time": "2016-10-19T17:31:16.757Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[]"}}, {"model": "admin.logentry", "pk": 17, "fields": {"action_time": "2016-10-19T17:31:19.400Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[{\"changed\": {\"fields\": [\"shortcode\"]}}]"}}, {"model": "admin.logentry", "pk": 18, "fields": {"action_time": "2016-10-19T21:37:16.369Z", "user": 1, "content_type": 7, "object_id": "1", "object_repr": "http://cfeblog.com/", "action_flag": 2, "change_message": "[{\"changed\": {\"fields\": [\"active\"]}}]"}}, {"model": "admin.logentry", "pk": 19, "fields": {"action_time": "2016-10-20T17:19:00.801Z", "user": 1, "content_type": 7, "object_id": "6", "object_repr": "http://cfeblog.com/2/", "action_flag": 1, "change_message": "[{\"added\": {}}]"}}, {"model": "admin.logentry", "pk": 20, "fields": {"action_time": "2016-10-20T23:34:48.079Z", "user": 1, "content_type": 7, "object_id": "8", "object_repr": "https://www.youtube.com/playlist?list=PLEsfXFp6DpzRB30gXPSwzAEQfqiUZkRsg", "action_flag": 2, "change_message": "[{\"changed\": {\"fields\": [\"shortcode\"]}}]"}}, {"model": "admin.logentry", "pk": 21, "fields": {"action_time": "2016-10-20T23:34:53.907Z", "user": 1, "content_type": 7, "object_id": "8", "object_repr": "https://www.youtube.com/playlist?list=PLEsfXFp6DpzRB30gXPSwzAEQfqiUZkRsg", "action_flag": 2, "change_message": "[{\"changed\": {\"fields\": [\"shortcode\"]}}]"}}, {"model": "admin.logentry", "pk": 22, "fields": {"action_time": "2017-04-18T18:09:26.995Z", "user": 2, "content_type": 11, "object_id": "1", "object_repr": "ArchLinux", "action_flag": 1, "change_message": "Added."}}, {"model": "admin.logentry", "pk": 23, "fields": {"action_time": "2017-04-18T18:09:36.535Z", "user": 2, "content_type": 11, "object_id": "2", "object_repr": "Python", "action_flag": 1, "change_message": "Added."}}, {"model": "admin.logentry", "pk": 24, "fields": {"action_time": "2017-04-18T18:09:45.033Z", "user": 2, "content_type": 11, "object_id": "3", "object_repr": "Django", "action_flag": 1, "change_message": "Added."}}, {"model": "admin.logentry", "pk": 25, "fields": {"action_time": "2017-04-18T18:09:57.199Z", "user": 2, "content_type": 11, "object_id": "4", "object_repr": "Pypi", "action_flag": 1, "change_message": "Added."}}, {"model": "admin.logentry", "pk": 26, "fields": {"action_time": "2017-04-18T18:10:04.257Z", "user": 2, "content_type": 11, "object_id": "5", "object_repr": "Git", "action_flag": 1, "change_message": "Added."}}, {"model": "admin.logentry", "pk": 27, "fields": {"action_time": "2017-04-18T18:10:14.075Z", "user": 2, "content_type": 11, "object_id": "6", "object_repr": "Gulp", "action_flag": 1, "change_message": "Added."}}, {"model": "admin.logentry", "pk": 28, "fields": {"action_time": "2017-04-21T11:02:13.924Z", "user": 2, "content_type": 14, "object_id": "7", "object_repr": "Differences between distribute, distutils, setuptools and distutils2?", "action_flag": 2, "change_message": "Changed slug and tags."}}, {"model": "admin.logentry", "pk": 29, "fields": {"action_time": "2017-04-21T11:02:48.076Z", "user": 2, "content_type": 14, "object_id": "6", "object_repr": "Python Packaging: Hate, hate, hate everywhere", "action_flag": 2, "change_message": "Changed slug and tags."}}, {"model": "admin.logentry", "pk": 30, "fields": {"action_time": "2017-04-21T11:03:15.071Z", "user": 2, "content_type": 14, "object_id": "4", "object_repr": "Django Anatomy", "action_flag": 2, "change_message": "Changed tags."}}, {"model": "admin.logentry", "pk": 31, "fields": {"action_time": "2017-04-21T11:03:27.455Z", "user": 2, "content_type": 14, "object_id": "3", "object_repr": "Gulp for Beginners", "action_flag": 2, "change_message": "Changed tags."}}, {"model": "admin.logentry", "pk": 32, "fields": {"action_time": "2017-04-21T11:03:43.782Z", "user": 2, "content_type": 14, "object_id": "2", "object_repr": "web pack", "action_flag": 2, "change_message": "Changed tags."}}, {"model": "admin.logentry", "pk": 33, "fields": {"action_time": "2017-04-21T11:03:53.179Z", "user": 2, "content_type": 14, "object_id": "1", "object_repr": "Git Guide", "action_flag": 2, "change_message": "Changed tags."}}]